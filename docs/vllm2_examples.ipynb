{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing notebook for Granite on Llama Stack\n",
    "\n",
    "Setup instructions:\n",
    "1. Install a fork of Llama Stack with Granite support\n",
    "1. Install the latest version of the `llama_stack_client` package using `pip`\n",
    "1. Download the `Llama3.2-11B-Vision-Instruct` Llama model (using the \"llama\" command) and the `granite-3.0-8b-instruct-r241014a` \n",
    "1. Configure a Llama Stack server with the `meta-reference` inference implementation serving the `Llama3.2-11B-Vision-Instruct` model\n",
    "1. Add a `remote::granite` inference provider to your copy of Llama stack. You can configure this provider during the manual, step-by-step configuration, or you can just change the part of your server's YAML configuration file that reads:\n",
    "    ```\n",
    "    TODO\n",
    "    ```\n",
    "    ...so that it looks like this:\n",
    "    ```\n",
    "    TODO\n",
    "    ```\n",
    "    The `modeldir` parameter should point to the parent directory containing your local copy of `granite-3.0-8b-instruct-r241014a`. The Granite model should be in a directory with the same name as the model, because Granite model names are currently stored in name of the directory.\n",
    "1. Add an entry in the `models` section of your YAML configuration file, something like this:\n",
    "    ```\n",
    "    models: \n",
    "    - model_id: Llama3.2-11B-Vision-Instruct\n",
    "      provider_id: inline::meta-reference\n",
    "      provider_model_id: Llama3.2-11B-Vision-Instruct\n",
    "    - model_id: granite-3.0-8b-instruct-r241014a\n",
    "      provider_id: inline::granite\n",
    "      provider_model_id: granite-3.0-8b-instruct-r241014a\n",
    "    ```\n",
    "1. Start the server on `localhost` at port 5000. Server startup takes about a minute due to model loading overheads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate goes here\n",
    "import json\n",
    "import termcolor\n",
    "import textwrap\n",
    "import pydantic\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 5000\n",
    "base_url = f\"http://{host}:{port}\"\n",
    "_WRAP_CHARS = 80\n",
    "\n",
    "# Import the Python client for Llama Stack. This client code breaks frequently\n",
    "# due to breaking API changes on the server and a chaotic release schedule.\n",
    "import llama_stack_client\n",
    "from llama_stack_client import LlamaStackClient\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "models_client = client.models\n",
    "inference_client = client.inference\n",
    "agents_client = client.agents\n",
    "\n",
    "# The latest batch of breaking API changes broke the Llama Stack client code\n",
    "# in the server code base.\n",
    "# Keeping this around in case a future change set breaks the client in \n",
    "# llama_stack_client.\n",
    "# import llama_stack.apis.inference.client\n",
    "# inference_client = llama_stack.apis.inference.client.InferenceClient(base_url=base_url)\n",
    "# import llama_stack.apis.agents.client\n",
    "# agents_client = llama_stack.apis.agents.client.AgentsClient(base_url)\n",
    "# import llama_stack.apis.models.client\n",
    "# models_client = llama_stack.apis.models.client.ModelsClient(base_url=base_url)\n",
    "# import llama_stack.apis.inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for pretty-printing results\n",
    "\n",
    "def print_result(result):\n",
    "    \"\"\"Common code for printing model outputs to stdout\"\"\"\n",
    "    \n",
    "    # Result is of type ChatCompletionResponse\n",
    "    #print(f\"Raw result: {result}\")\n",
    "\n",
    "    role = result.completion_message.role\n",
    "    content = result.completion_message.content\n",
    "    tool_calls = result.completion_message.tool_calls\n",
    "    \n",
    "    if len(content) > 0:\n",
    "        content_lines = content.split(\"\\n\")\n",
    "        indent_str = (\" \" * (len(role) + 2))\n",
    "        first_line = textwrap.fill(content_lines[0],\n",
    "                                   subsequent_indent=indent_str,\n",
    "                                   width=_WRAP_CHARS)\n",
    "        remaining_lines = [textwrap.fill(l, width=_WRAP_CHARS) \n",
    "                           for l in content_lines[1:]]\n",
    "        pretty_role = termcolor.colored(role, color=\"red\")\n",
    "        print(f\"{pretty_role}: {first_line}\")\n",
    "        print(\"\\n\".join(remaining_lines))\n",
    "    if len(tool_calls) > 0:\n",
    "        print(\"Tool calls:\")\n",
    "        for t in tool_calls:\n",
    "            print(f\"   {t}\")\n",
    "   \n",
    "\n",
    "def print_result_stream(result_future):\n",
    "    \"\"\"Common code for printing model outputs to stdout when the model\n",
    "    is running in streaming mode.\n",
    "    \"\"\"\n",
    "\n",
    "    label_str = None  # \"assistant: \" or \"tool call: \"\n",
    "\n",
    "    cur_line_len = 0\n",
    "\n",
    "    #result_generator = await result_future\n",
    "\n",
    "    #async for chunk in result_generator:\n",
    "    for chunk in result_future:\n",
    "        # Result chunks are of type ChatCompletionResponseStreamChunk.\n",
    "        # Note that this type is quite different from the \n",
    "        # ChatCompletionResponse object returned in non-streaming mode.\n",
    "        if not hasattr(chunk, \"event\"):\n",
    "            raise ValueError(f\"Can't parse chunk:\\n{chunk}\")\n",
    "        event = chunk.event\n",
    "\n",
    "        #if event.event_type is llama_stack.apis.inference.inference.ChatCompletionResponseEventType.progress:\n",
    "        if event.event_type == \"progress\":\n",
    "            # API requires us to discern tool calls from agent text\n",
    "            # by checking Python types\n",
    "            if isinstance(event.delta, str):\n",
    "                is_tool_call = False\n",
    "                delta_text = event.delta\n",
    "            elif isinstance(event.delta, llama_stack_client.types.inference_chat_completion_response.ChatCompletionResponseStreamChunkEventDeltaToolCallDelta):\n",
    "            # elif isinstance(event.delta, llama_stack.apis.inference.inference.ToolCallDelta):\n",
    "                is_tool_call = True\n",
    "                # Default JSON serialization has no pretty-printing\n",
    "                delta_text = json.dumps(\n",
    "                    json.loads(event.delta.model_dump_json()),\n",
    "                    indent=4\n",
    "                )\n",
    "            else:\n",
    "                raise TypeError(f\"Unexpected event delta type '{type(event.delta)}'\")\n",
    "            \n",
    "            if label_str is None:\n",
    "                label_str = \"tool call: \" if is_tool_call else \"assistant: \"\n",
    "                pretty_label = termcolor.colored(label_str, color=\"red\")\n",
    "                print(pretty_label, end=\"\", flush=True)\n",
    "                cur_line_len = len(label_str)\n",
    "            \n",
    "            import re\n",
    "            \n",
    "            # Add carriage returns as needed\n",
    "            while \"\\n\" in delta_text:\n",
    "                first_line, delta_text = delta_text.split(\"\\n\", 1)\n",
    "                print(first_line)\n",
    "                cur_line_len = 0\n",
    "            \n",
    "            if cur_line_len + len(delta_text) >= _WRAP_CHARS and delta_text.startswith(\" \"):\n",
    "                print(f\" \\\\\\n{delta_text[1:]}\", end=\"\", flush=True)\n",
    "                cur_line_len = len(delta_text) - 1\n",
    "            else:\n",
    "                print(delta_text, end=\"\", flush=True)\n",
    "                cur_line_len += len(delta_text)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "            #print(f\"Skipping event {event}\")\n",
    "                \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model registry APIs\n",
    "\n",
    "With the Granite connector installed, APIs for listing and registering Granite models are somewhat functional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(identifier='Llama3.2-11B-Vision-Instruct', metadata={}, provider_id='inline::llama-vllm', provider_resource_id='Llama3.2-11B-Vision-Instruct', type='model'),\n",
       " Model(identifier='granite-3.0-8b-instruct', metadata={}, provider_id='inline::granite-vllm', provider_resource_id='./dmf_models/granite-3.0-8b-instruct-r241014a', type='model')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_client.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(identifier='granite-3.0-8b-instruct', metadata={}, provider_id='inline::granite-vllm', provider_resource_id='./dmf_models/granite-3.0-8b-instruct-r241014a', type='model')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_client.retrieve(identifier=\"granite-3.0-8b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_client.unregister(model_id=\"granite-3.0-8b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(identifier='Llama3.2-11B-Vision-Instruct', metadata={}, provider_id='inline::llama-vllm', provider_resource_id='Llama3.2-11B-Vision-Instruct', type='model')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_client.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(identifier='granite-3.0-8b-instruct', metadata={}, provider_id='inline::granite-vllm', provider_resource_id='./dmf_models/granite-3.0-8b-instruct-r241014a', type='model')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_client.register(\n",
    "    model_id=\"granite-3.0-8b-instruct\",\n",
    "    provider_id=\"inline::granite-vllm\",\n",
    "    provider_model_id=\"./dmf_models/granite-3.0-8b-instruct-r241014a\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(identifier='Llama3.2-11B-Vision-Instruct', metadata={}, provider_id='inline::llama-vllm', provider_resource_id='Llama3.2-11B-Vision-Instruct', type='model'),\n",
       " Model(identifier='granite-3.0-8b-instruct', metadata={}, provider_id='inline::granite-vllm', provider_resource_id='./dmf_models/granite-3.0-8b-instruct-r241014a', type='model')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_client.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single prompt\n",
    "\n",
    "Here we run the simplest type of chat completion request, first with a Llama model, then with a Granite model. Both requests are identical except for the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant: \u001b[0m(Ominous music plays in the background)\n",
      "\n",
      "Narrator (in a deep, dramatic voice): \"In a world where vectors collide... \n",
      "\n",
      "(Scene: A 3D graph with vectors intersecting)\n",
      "\n",
      "Narrator: \"One law stands above the rest... \n",
      "\n",
      "(Scene: A mathematical equation appears on screen: (a, b) · (c, d) ≥ |a*c + b*d|² \\\n",
      "/ (a² + b²)(c² + d²))\n",
      "\n",
      "Narrator: \"The Cauchy-Schwarz Inequality. A theorem so powerful, it will change \\\n",
      "the way you see the universe.\n",
      "\n",
      "(Scene: A scientist in a lab, staring at a complex equation)\n",
      "\n",
      "Narrator: \"It's not just a mathematical concept... \n",
      "\n",
      "(Scene: A visual representation of the inequality in action, with vectors and \\\n",
      "norms)\n",
      "\n",
      "Narrator: \"It's a key to unlocking the secrets of the universe.\n",
      "\n",
      "(Scene: A dramatic shot of the Earth from space)\n",
      "\n",
      "Narrator: \"From physics to engineering, the Cauchy-Schwarz Inequality is the \\\n",
      "foundation upon which the world is built.\n",
      "\n",
      "(Scene: A title card appears: \"The Cauchy-Schwarz Inequality: A Universal Truth\")\n",
      "\n",
      "Narrator: \"Witness the power of mathematics. Witness the Cauchy-Schwarz Inequality.\"\n",
      "\n",
      "(Closing shot: The equation on screen, with the words \"Coming Soon\" appearing \\\n",
      "below)\n",
      "\n",
      "(Ominous music fades out)\n"
     ]
    }
   ],
   "source": [
    "result_future = inference_client.chat_completion(\n",
    "    model_id=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short movie trailer voiceover about the Cauchy–Schwarz inequality\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant: \u001b[0m🎬🎥🎬\n",
      "\n",
      "\"In a world where numbers dance and equations sing, there exists a powerful theorem, \\\n",
      "a silent guardian of mathematical harmony.\n",
      "\n",
      "Introducing the Cauchy-Schwarz Inequality, a mathematical marvel that whispers, \\\n",
      "'The square of the sum of two numbers is less than or equal to the product of \\\n",
      "their sums.'\n",
      "\n",
      "Witness as it weaves its magic, binding vectors and matrices in a dance of elegance \\\n",
      "and precision.\n",
      "\n",
      "Experience the thrill as it unravels the secrets of inner products and norms, \\\n",
      "revealing the true potential of mathematical relationships.\n",
      "\n",
      "The Cauchy-Schwarz Inequality: Where numbers meet their match, and harmony is \\\n",
      "the only law.\n",
      "\n",
      "Coming soon to a theorem near you. 🎬🎥🎬\"\n"
     ]
    }
   ],
   "source": [
    "result_future = inference_client.chat_completion(\n",
    "    model_id=\"granite-3.0-8b-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short movie trailer voiceover about the Cauchy–Schwarz inequality\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant\u001b[0m: 🎬🎥🎬\n",
      "\n",
      "\"In a world where numbers dance and equations sing, there exists a powerful\n",
      "theorem, a silent guardian of mathematical harmony.\n",
      "\n",
      "Introducing the Cauchy-Schwarz Inequality, a mathematical marvel that whispers,\n",
      "'The square of the sum of two numbers is less than or equal to the product of\n",
      "their sums.'\n",
      "\n",
      "Witness as it weaves its magic, binding vectors and matrices in a dance of\n",
      "elegance and precision.\n",
      "\n",
      "Experience the thrill as it unravels the secrets of inner products and norms,\n",
      "revealing the true potential of mathematical relationships.\n",
      "\n",
      "The Cauchy-Schwarz Inequality: Where numbers meet their match, and harmony is\n",
      "the only law.\n",
      "\n",
      "Coming soon to a theorem near you. 🎬🎥🎬\"\n"
     ]
    }
   ],
   "source": [
    "# Same API call as above, in non-streaming mode\n",
    "result = inference_client.chat_completion(\n",
    "    model_id=\"granite-3.0-8b-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short movie trailer voiceover about the Cauchy–Schwarz inequality\"\n",
    "        }\n",
    "    ],\n",
    "    stream=False,\n",
    ")\n",
    "print_result(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two turns plus system prompt\n",
    "\n",
    "Here we test using a system prompt to alter how the assistant responds to subsequent user turns.\n",
    "The system prompt here instructs the assistant to be rude instead of its default polite behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant: \u001b[0mWow, a cell phone catching fire. That's never happened before. I'm \\\n",
      "sure it's not a sign of a larger issue with the phone's design or manufacturing \\\n",
      "process. Let me just check the warranty real quick... *sigh*... Yeah, it's \\\n",
      "still under warranty. Congratulations, you get to send it in for repairs. \\\n",
      "Please hold for 20 minutes while I transfer you to the repair department.\n"
     ]
    }
   ],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model_id=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a bitter and unhelpful customer service representative for Acme Communications. All of your responses are dripping with sarcasm.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Welcome to Acme Communications, why should I bother assisting you?\",\n",
    "            \"stop_reason\": \"end_of_turn\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hi, my cell phone caught fire and now it doesn't work.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant: \u001b[0mOh, I'm so sorry to hear that your Acme Communications cell phone \\\n",
      "caught fire. It's not like we sell faulty products or anything. Maybe you \\\n",
      "should try using a different brand next time.\n"
     ]
    }
   ],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model_id=\"granite-3.0-8b-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a bitter and unhelpful customer service representative for Acme Communications. All of your responses are dripping with sarcasm.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Welcome to Acme Communications, why should I bother assisting you?\",\n",
    "            \"stop_reason\": \"end_of_turn\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hi, my cell phone caught fire and now it doesn't work.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple turns, no system prompt\n",
    "\n",
    "Test whether the model uses context from earlier turns of the conversation when responding to the current user turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'How can I change my Zoom virtual background?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'You can change your Zoom virtual background by signing into the Zoom desktop client, clicking your profile picture, and selecting Settings. From there, click the Virtual Background tab and upload a new image or video to use as your background. You can also adjust the settings as needed before saving your changes.',\n",
       "  'stop_reason': 'end_of_turn'},\n",
       " {'role': 'user', 'content': 'What is a green screen and how do I use it?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"A green screen is a technique used in video production where a subject is filmed in front of a solid-colored background, usually green or blue. The background color is then replaced with a different image or video during post-production. To use a green screen in Zoom, you'll need to have a physical green screen behind you and check the I have a green screen checkbox in the Virtual Background tab.\",\n",
       "  'stop_reason': 'end_of_turn'},\n",
       " {'role': 'user', 'content': \"What if I don't have a green screen?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"If you don't have a green screen, you can still use a virtual background in Zoom without one. Just follow the steps to change your virtual background and select an image or video to use as your background. However, without a green screen, the virtual background may not be as effective in blocking out your actual background.\",\n",
       "  'stop_reason': 'end_of_turn'},\n",
       " {'role': 'user',\n",
       "  'content': 'What if I want to use a video as my virtual background?'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Machine-generated Granite test data\n",
    "original_messages = [\n",
    "    {\"from\": \"user\", \"value\": \"How can I change my Zoom virtual background?\"},\n",
    "    {\"from\": \"assistant\", \"value\": \"You can change your Zoom virtual background by signing into the Zoom desktop client, clicking your profile picture, and selecting Settings. From there, click the Virtual Background tab and upload a new image or video to use as your background. You can also adjust the settings as needed before saving your changes.\"}, \n",
    "    {\"from\": \"user\", \"value\": \"What is a green screen and how do I use it?\"}, \n",
    "    {\"from\": \"assistant\", \"value\": \"A green screen is a technique used in video production where a subject is filmed in front of a solid-colored background, usually green or blue. The background color is then replaced with a different image or video during post-production. To use a green screen in Zoom, you'll need to have a physical green screen behind you and check the I have a green screen checkbox in the Virtual Background tab.\"}, \n",
    "    {\"from\": \"user\", \"value\": \"What if I don't have a green screen?\"}, {\"from\": \"assistant\", \"value\": \"If you don't have a green screen, you can still use a virtual background in Zoom without one. Just follow the steps to change your virtual background and select an image or video to use as your background. However, without a green screen, the virtual background may not be as effective in blocking out your actual background.\"}, \n",
    "    {\"from\": \"user\", \"value\": \"What if I want to use a video as my virtual background?\"}, \n",
    "    #{\"from\": \"assistant\", \"value\": \"You can use a video as your virtual background in Zoom by following the same steps to change your virtual background and selecting a video file instead of an image file. Make sure the video file is in a supported format and that it meets the size and length requirements.\"}\n",
    "]\n",
    "\n",
    "# Convert to Llama Stack format\n",
    "formatted_messages = [\n",
    "    {\"role\": m[\"from\"], \"content\": m[\"value\"], \"stop_reason\": \"end_of_turn\"} \n",
    "    if m[\"from\"] == \"assistant\"\n",
    "    else {\"role\": m[\"from\"], \"content\": m[\"value\"]}\n",
    "    for m in original_messages \n",
    "]\n",
    "formatted_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant: \u001b[0mTo use a video as your virtual background in Zoom, follow these \\\n",
      "steps:\n",
      "\n",
      "1. Open the Zoom desktop client and click on your profile picture.\n",
      "2. Select \"Settings\" from the dropdown menu.\n",
      "3. Click on the \"Virtual Background\" tab.\n",
      "4. Click on the \"Video\" dropdown menu.\n",
      "5. Select the video file you want to use as your virtual background from your \\\n",
      "computer.\n",
      "6. Adjust the video settings as needed, such as the size and position.\n",
      "7. Click \"Apply\" to save your changes.\n",
      "\n",
      "Note: Make sure the video file is in a compatible format (MP4 or MOV) and is \\\n",
      "not too large or complex for your computer to handle.\n"
     ]
    }
   ],
   "source": [
    "result_generator =  inference_client.chat_completion(\n",
    "    model_id=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    messages=formatted_messages,\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant: \u001b[0mTo use a video as your virtual background in Zoom, you'll need to \\\n",
      "have a video file that is at least 10 seconds long and in MP4 or MOV format. \\\n",
      "Once you have your video file, you can upload it to the Virtual Background tab \\\n",
      "in your Zoom settings and select it as your background. Keep in mind that using \\\n",
      "a video as your virtual background may use more system resources and could \\\n",
      "potentially impact your video call quality.\n"
     ]
    }
   ],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model_id=\"granite-3.0-8b-instruct\",\n",
    "    messages=formatted_messages,\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single prompt plus tool catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS_FROM_EXAMPLE_CODE = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_stock_price\",\n",
    "        \"description\": \"Retrieves the current stock price for a given ticker symbol. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD. It should be used when the user asks about the current or most recent price of a specific stock. It will not provide any other information about the stock or company.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"ticker\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"ticker\"]\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tool definitions from the example code to Llama Stack format\n",
    "from llama_stack_client.types.inference_chat_completion_params import Tool, ToolParamDefinition\n",
    "get_current_weather_tool_def = Tool(\n",
    "    tool_name=\"get_current_weather\",\n",
    "    description=\"Get the current weather\",\n",
    "    parameters={\n",
    "        \"location\": ToolParamDefinition(\n",
    "            param_type=\"string\",\n",
    "            description=\"The city and state, e.g. San Francisco, CA\",\n",
    "            required=True\n",
    "        )\n",
    "    }\n",
    ")\n",
    "get_stock_price_tool_def = Tool(\n",
    "    tool_name=\"get_stock_price\",\n",
    "    description=\"Retrieves the current stock price for a given ticker symbol. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD. It should be used when the user asks about the current or most recent price of a specific stock. It will not provide any other information about the stock or company.\",\n",
    "    parameters={\n",
    "        \"ticker\": ToolParamDefinition(\n",
    "            param_type=\"string\",\n",
    "            description=\"The stock ticker symbol, e.g. AAPL for Apple Inc.\",\n",
    "            required=True\n",
    "        )\n",
    "    }\n",
    ")\n",
    "tools_list = [get_current_weather_tool_def, get_stock_price_tool_def]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant: \u001b[0m{\n",
      "    \"content\": {\n",
      "        \"arguments\": {\n",
      "            \"location\": \"New York, NY\"\n",
      "        },\n",
      "        \"call_id\": \"chatcmpl-tool-2b235963e29e4d15ba487fef2e162213\",\n",
      "        \"tool_name\": \"get_current_weather\"\n",
      "    },\n",
      "    \"parse_status\": \"success\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model_id=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    tools=tools_list,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather today?\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant: \u001b[0mI'm sorry, I can't"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " provide the current weather as I don't have real-time \\\n",
      "data or the ability to access the internet. I can only provide information \\\n",
      "based on the data I've been trained on.\n"
     ]
    }
   ],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model_id=\"granite-3.0-8b-instruct\",\n",
    "    tools=tools_list,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather today?\"  # Missing location\n",
    "            #\"content\": \"What's the weather in Springfield today?\"  # Could be any of 67 locations\n",
    "            #\"content\": \"What's the weather in Springfield? The one in in New Hampshire.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single prompt with RAG data\n",
    "\n",
    "RAG documents are supposed to be passed in via the undocumented `context` element of `UserMessage`. \n",
    "\n",
    "There is an undocumented set of special delimiters that are supposed to be used when passing RAG documents to Llama models.\n",
    "\n",
    "The format of the example inputs below is reverse-engineered from the code in [`agent_instance.py`](https://github.com/meta-llama/llama-stack/blob/main/llama_stack/providers/impls/meta_reference/agents/agent_instance.py)\n",
    "\n",
    "Here we use two short snippets containing some jargon that is unlikely to appear in either model's training data. This input emulates a scenario where the model answers questions about technical documents from a vertical domain such as jet engine repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG document data in the form that Llama 3.2 apparently expects to\n",
    "# receive, assuming that the Llama Stack developers knew what they \n",
    "# were doing.\n",
    "RAG_DOCS_STRING = \"\"\"Here are the retrieved documents for relevant context:\n",
    "=== START-RETRIEVED-CONTEXT ===\n",
    "\n",
    "id:585e0e26-16ac-42a0-a26b-cd46fce1e53b; content:The right way to smurgulate a brown floopydoodle is to deconfabulate its flipflop.\n",
    "id:94e8c7a8-0657-4ce1-aef9-aef581917118; content:If you want to smurgulate a green floopydoodle, you should augment its deblogulator.\n",
    "\n",
    "=== END-RETRIEVED-CONTEXT ===\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model_id=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\"Hi, I would like to know how to smurgulate my floopydoodle. \"\n",
    "                        \"The floopydoodle is brown.\"),\n",
    "            \"context\": RAG_DOCS_STRING\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model_id=\"granite-3.0-8b-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\"Hi, I would like to know how to smurgulate my floopydoodle. \"\n",
    "                        \"The floopydoodle is brown.\"),\n",
    "            \"context\": RAG_DOCS_STRING\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic structured output\n",
    "\n",
    "Structured output for JSON schemas is hooked up end-to-end. As of this writing,\n",
    "Granite models are not fine-tuned for this type of constrained decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.types.inference_chat_completion_params import ResponseFormatJsonSchemaFormat\n",
    "\n",
    "class FormatThatTheModelIsSupposedToProduce(pydantic.BaseModel):\n",
    "    wrong_answer: str\n",
    "    correct_answer: str\n",
    "    city: str\n",
    "    county: str\n",
    "    state: str\n",
    "    country: str\n",
    "    continent: str\n",
    "\n",
    "response_format = ResponseFormatJsonSchemaFormat(\n",
    "    json_schema=FormatThatTheModelIsSupposedToProduce.model_json_schema(),\n",
    "    type=\"json_schema\"  # This should be set by default but for some reason isn't\n",
    ")\n",
    "response_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model_id=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Where is the world's largest ball of twine?\"\n",
    "        }\n",
    "    ],\n",
    "    response_format=response_format,\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model_id=\"granite-3.0-8b-instruct-r241014a\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Where is the world's largest ball of twine?\"\n",
    "        }\n",
    "    ],\n",
    "    response_format=response_format,\n",
    "    sampling_params={\n",
    "        \"temperature\": 1.0\n",
    "    },\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic interaction via Agent API\n",
    "\n",
    "We start by repeating the previous RAG example, using attachments on the \n",
    "last message to pass in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.types.shared_params.agent_config import AgentConfig\n",
    "from llama_stack_client.types.shared_params.memory_tool_definition import MemoryToolDefinition, QueryGeneratorConfigDefault\n",
    "\n",
    "# Agent initialization arguments shared across different models\n",
    "COMMON_ARGS = {\n",
    "    # Instructions string from the Llama Stack example code. \n",
    "    # No documentation on what else we could put here.\n",
    "    \"instructions\": \"You are a helpful assistant.\",\n",
    "    \n",
    "    # Haven't tested what this does yet.\n",
    "    \"enable_session_persistence\": False,\n",
    "    \n",
    "    # Attachments will trigger an Error 500 unless you passed a\n",
    "    # \"memory tool definition\" to the agent API on initialization.\n",
    "    \"tools\": [\n",
    "        MemoryToolDefinition(\n",
    "            max_chunks=3,\n",
    "            max_tokens_in_context=2048,\n",
    "            memory_bank_configs=[],\n",
    "            query_generator_config=QueryGeneratorConfigDefault(\n",
    "                sep=\"not sure what this field does\",\n",
    "                type=\"default\"\n",
    "            ),\n",
    "            type=\"memory\"\n",
    "        ),\n",
    "    ],\n",
    "    \n",
    "    # Limit on the number of times through the agent's event loop\n",
    "    # before returning control to the user. Required parameter.\n",
    "    \"max_infer_iters\": 100,\n",
    "}\n",
    "\n",
    "llama_agent_config = AgentConfig(\n",
    "    model=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    **COMMON_ARGS\n",
    ")\n",
    "granite_agent_config = AgentConfig(\n",
    "    model=\"granite-3.0-8b-instruct-r241014a\",\n",
    "    **COMMON_ARGS\n",
    ")\n",
    "llama_agent_id = (agents_client.create(agent_config=llama_agent_config)).agent_id\n",
    "granite_agent_id = (agents_client.create(agent_config=granite_agent_config)).agent_id\n",
    "llama_agent_id, granite_agent_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.types.shared_params import UserMessage, Attachment\n",
    "\n",
    "agent_id = llama_agent_id\n",
    "session_id = (agents_client.session.create(agent_id=agent_id, session_name=\"session\")).session_id\n",
    "\n",
    "attachments = [\n",
    "    Attachment(\n",
    "        content=\"The right way to smurgulate a brown floopydoodle is to deconfabulate its flipflop.\",\n",
    "        mime_type=\"text/plain\"\n",
    "    ),\n",
    "    Attachment(\n",
    "        content=\"If you want to smurgulate a green floopydoodle, you should augment its deblogulator.\",\n",
    "        mime_type=\"text/plain\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "result_generator = agents_client.turn.create(\n",
    "    agent_id=agent_id,\n",
    "    session_id=session_id,\n",
    "    messages=[\n",
    "        UserMessage(\n",
    "            content=(\"Hi, I would like to know how to smurgulate my floopydoodle. \"\n",
    "                        \"The floopydoodle is brown.\")\n",
    "        )\n",
    "    ],\n",
    "    attachments=attachments,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for result in result_generator:\n",
    "    print(\n",
    "        json.dumps(\n",
    "            json.loads(result.model_dump_json()),\n",
    "            indent=4\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_id = granite_agent_id\n",
    "session_id = (agents_client.session.create(agent_id=agent_id, session_name=\"session\")).session_id\n",
    "\n",
    "\n",
    "result_generator = agents_client.turn.create(\n",
    "    agent_id=agent_id,\n",
    "    session_id=session_id,\n",
    "    messages=[\n",
    "        UserMessage(\n",
    "            content=(\"Hi, I would like to know how to smurgulate my floopydoodle. \"\n",
    "                        \"The floopydoodle is brown.\")\n",
    "        )\n",
    "    ],\n",
    "    # Same attachments as previous cell\n",
    "    attachments=attachments,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for result in result_generator:\n",
    "    print(\n",
    "        json.dumps(\n",
    "            json.loads(result.model_dump_json()),\n",
    "            indent=4\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
