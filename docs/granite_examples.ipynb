{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing notebook for Granite on Llama Stack\n",
    "\n",
    "Setup instructions:\n",
    "1. Install a fork of Llama Stack with Granite support\n",
    "1. Install the latest version of the `llama_stack_client` package using `pip`\n",
    "1. Download the `Llama3.2-11B-Vision-Instruct` Llama model (using the \"llama\" command) and the `granite-3.0-8b-instruct-r241014a` \n",
    "1. Configure a Llama Stack server with the `meta-reference` inference implementation serving the `Llama3.2-11B-Vision-Instruct` model\n",
    "1. Add a `remote::granite` inference provider to your copy of Llama stack. You can configure this provider during the manual, step-by-step configuration, or you can just change the part of your server's YAML configuration file that reads:\n",
    "    ```\n",
    "    providers:\n",
    "      inference:\n",
    "      - provider_id: inline::meta-reference\n",
    "        provider_type: inline::meta-reference\n",
    "        config:\n",
    "          model: Llama3.2-11B-Vision\n",
    "          torch_seed: null\n",
    "          max_seq_len: 4096\n",
    "          max_batch_size: 1\n",
    "          create_distributed_process_group: true\n",
    "          checkpoint_dir: null\n",
    "    ```\n",
    "    ...so that it looks like this:\n",
    "    ```\n",
    "    providers:\n",
    "      inference:\n",
    "      - provider_id: inline::meta-reference\n",
    "        provider_type: inline::meta-reference\n",
    "        config:\n",
    "          model: Llama3.2-11B-Vision\n",
    "          torch_seed: null\n",
    "          max_seq_len: 4096\n",
    "          max_batch_size: 1\n",
    "          create_distributed_process_group: true\n",
    "          checkpoint_dir: null\n",
    "      - provider_id: inline::granite\n",
    "        provider_type: inline::granite\n",
    "        config:\n",
    "          modeldir: dmf_models\n",
    "          backend: vllm\n",
    "          preload_model_name: granite-3.0-8b-instruct-r241014a\n",
    "    ```\n",
    "    The `modeldir` parameter should point to the parent directory containing your local copy of `granite-3.0-8b-instruct-r241014a`. The Granite model should be in a directory with the same name as the model, because Granite model names are currently stored in name of the directory.\n",
    "1. Add an entry in the `models` section of your YAML configuration file, something like this:\n",
    "    ```\n",
    "    models: \n",
    "    - model_id: Llama3.2-11B-Vision-Instruct\n",
    "      provider_id: inline::meta-reference\n",
    "      provider_model_id: Llama3.2-11B-Vision-Instruct\n",
    "    - model_id: granite-3.0-8b-instruct-r241014a\n",
    "      provider_id: inline::granite\n",
    "      provider_model_id: granite-3.0-8b-instruct-r241014a\n",
    "    ```\n",
    "1. Start the server on `localhost` at port 5000. Server startup takes about a minute due to model loading overheads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate goes here\n",
    "import json\n",
    "import termcolor\n",
    "import textwrap\n",
    "import pydantic\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 5000\n",
    "base_url = f\"http://{host}:{port}\"\n",
    "_WRAP_CHARS = 80\n",
    "\n",
    "# Import the Python client for Llama Stack. This client code breaks frequently\n",
    "# due to breaking API changes on the server and a chaotic release schedule.\n",
    "from llama_stack_client import LlamaStackClient\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "models_client = client.models\n",
    "inference_client = client.inference\n",
    "agents_client = client.agents\n",
    "\n",
    "# The latest batch of breaking API changes broke the Llama Stack client code\n",
    "# in the server.\n",
    "# Alternate code reverse-ingineered from Llama Stack source\n",
    "# import llama_stack.apis.inference.client\n",
    "# inference_client = llama_stack.apis.inference.client.InferenceClient(base_url=base_url)\n",
    "# import llama_stack.apis.agents.client\n",
    "# agents_client = llama_stack.apis.agents.client.AgentsClient(base_url)\n",
    "# import llama_stack.apis.models.client\n",
    "# models_client = llama_stack.apis.models.client.ModelsClient(base_url=base_url)\n",
    "# import llama_stack.apis.inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for pretty-printing results\n",
    "\n",
    "async def print_result(result_future_future):\n",
    "    \"\"\"Common code for printing model outputs to stdout\"\"\"\n",
    "    # Inference APIs currently return a corutine that returns a corutine\n",
    "    # when called in non-streaming mode.\n",
    "    result_future = await result_future_future\n",
    "    result = await result_future\n",
    "    \n",
    "    # Result is of type ChatCompletionResponse\n",
    "    #print(f\"Raw result: {result}\")\n",
    "\n",
    "    role = result.completion_message.role\n",
    "    content = result.completion_message.content\n",
    "    tool_calls = result.completion_message.tool_calls\n",
    "    \n",
    "    if len(content) > 0:\n",
    "        content_lines = content.split(\"\\n\")\n",
    "        indent_str = (\" \" * (len(role) + 2))\n",
    "        first_line = textwrap.fill(content_lines[0],\n",
    "                                   subsequent_indent=indent_str,\n",
    "                                   width=_WRAP_CHARS)\n",
    "        remaining_lines = [textwrap.fill(l, width=_WRAP_CHARS) \n",
    "                           for l in content_lines[1:]]\n",
    "        pretty_role = termcolor.colored(role, color=\"red\")\n",
    "        print(f\"{pretty_role}: {first_line}\")\n",
    "        print(\"\\n\".join(remaining_lines))\n",
    "    if len(tool_calls) > 0:\n",
    "        print(\"Tool calls:\")\n",
    "        for t in tool_calls:\n",
    "            print(f\"   {t}\")\n",
    "   \n",
    "\n",
    "def print_result_stream(result_future):\n",
    "    \"\"\"Common code for printing model outputs to stdout when the model\n",
    "    is running in streaming mode.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_str = None  # \"assistant: \" or \"tool call: \"\n",
    "    \n",
    "    cur_line_len = 0\n",
    "    \n",
    "    #result_generator = await result_future\n",
    "    \n",
    "    #async for chunk in result_generator:\n",
    "    for chunk in result_future:\n",
    "        # Result chunks are of type ChatCompletionResponseStreamChunk.\n",
    "        # Note that this type is quite different from the \n",
    "        # ChatCompletionResponse object returned in non-streaming mode.\n",
    "        event = chunk.event\n",
    "    \n",
    "        #if event.event_type is llama_stack.apis.inference.inference.ChatCompletionResponseEventType.progress:\n",
    "        if event.event_type == \"progress\":\n",
    "            # API requires us to discern tool calls from agent text\n",
    "            # by checking Python types\n",
    "            if isinstance(event.delta, str):\n",
    "                is_tool_call = False\n",
    "                delta_text = event.delta\n",
    "            # elif isinstance(event.delta, llama_stack.apis.inference.inference.ToolCallDelta):\n",
    "            #     is_tool_call = True\n",
    "            #     # Default JSON serialization has no pretty-printing\n",
    "            #     delta_text = json.dumps(\n",
    "            #         json.loads(event.delta.model_dump_json()),\n",
    "            #         indent=4\n",
    "            #     )\n",
    "            else:\n",
    "                raise TypeError(f\"Unexpected event delta type '{type(event.delta)}'\")\n",
    "            \n",
    "            if label_str is None:\n",
    "                label_str = \"tool call: \" if is_tool_call else \"assistant: \"\n",
    "                pretty_label = termcolor.colored(label_str, color=\"red\")\n",
    "                print(pretty_label, end=\"\", flush=True)\n",
    "                cur_line_len = len(label_str)\n",
    "            \n",
    "            # Add carriage returns as needed\n",
    "            while \"\\n\" in delta_text:\n",
    "                first_line, delta_text = delta_text.split(\"\\n\", 1)\n",
    "                print(first_line)\n",
    "                cur_line_len = 0\n",
    "            \n",
    "            if cur_line_len + len(delta_text) >= _WRAP_CHARS and delta_text.startswith(\" \"):\n",
    "                print(f\" \\\\\\n{delta_text[1:]}\", end=\"\", flush=True)\n",
    "                cur_line_len = len(delta_text) - 1\n",
    "            else:\n",
    "                print(delta_text, end=\"\", flush=True)\n",
    "                cur_line_len += len(delta_text)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "            #print(f\"Skipping event {event}\")\n",
    "                \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model registry APIs\n",
    "\n",
    "With the Granite connector installed, APIs for listing and registering Granite models are somewhat functional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell no longer works with the latest batch of breaking API changes\n",
    "\n",
    "# List all available models. Should show both Llama and Granite models\n",
    "#await models_client.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell no longer works with the latest batch of breaking API changes\n",
    "\n",
    "#await models_client.get_model('granite-8b-code-instruct-128k-r241015a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell no longer works with the latest batch of breaking API changes\n",
    "\n",
    "# We can also register Granite models. This call should be a no-op, since the\n",
    "# model gets registered when the connector starts up.\n",
    "# await models_client.register_model(\n",
    "#     llama_stack.apis.models.ModelDefWithProvider(\n",
    "#         # Name used in URLs\n",
    "#         identifier=\"granite-3.0-8b-instruct-r241014a\",\n",
    "        \n",
    "#         # Name used by the inference provider\n",
    "#         llama_model=\"granite-3.0-8b-instruct-r241014a\",\n",
    "        \n",
    "#         # Should match field with same name in server's YAML config file\n",
    "#         provider_id=\"remote::granite\" \n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single prompt\n",
    "\n",
    "Here we run the simplest type of chat completion request, first with a Llama model, then with a Granite model. Both requests are identical except for the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant: \u001b[0m(Ominous music starts playing)\n",
      "\n",
      "Narrator (in a deep, dramatic voice): \"In a world where vectors collide, one \\\n",
      "truth stands above the rest. A boundary that separates the possible from the \\\n",
      "impossible.\"\n",
      "\n",
      "(Visuals of vectors intersecting, with a red line marking the limit)\n",
      "\n",
      "Narrator: \"It's the Cauchy-Schwarz inequality, the ultimate test of power and \\\n",
      "precision.\"\n",
      "\n",
      "(Visuals of mathematicians working, with equations flashing on screen)\n",
      "\n",
      "Narrator: \"A mathematical law that governs the inner workings of our universe, \\\n",
      "from the smallest particles to the vast expanse of space.\"\n",
      "\n",
      "(Visuals of a dot product, with the numbers increasing)\n",
      "\n",
      "Narrator: \"It's a delicate balance, a dance of numbers and variables, where the \\\n",
      "slightest misstep can lead to disaster.\"\n",
      "\n",
      "(Visuals of a vector going beyond the limit, with a dramatic explosion)\n",
      "\n",
      "Narrator: \"But what happens when the limits are pushed to the extreme? When the \\\n",
      "vectors collide, and the inequality is broken?\"\n",
      "\n",
      "(Visuals of a mathematician looking shocked, with a hint of excitement)\n",
      "\n",
      "Narrator: \"Find out, in the thrilling tale of the Cauchy-Schwarz inequality. A \\\n",
      "story of power, precision, and the ultimate test of mathematical might.\"\n",
      "\n",
      "(Title card: \"Cauchy-Schwarz: The Ultimate Limit\")\n",
      "\n",
      "Narrator: \"Coming soon, to a screen near you.\"\n",
      "\n",
      "(Music ends with a dramatic flourish)\n"
     ]
    }
   ],
   "source": [
    "result_future = inference_client.chat_completion(\n",
    "    model_id=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short movie trailer voiceover about the Cauchy–Schwarz inequality\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print_result_stream(result_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31massistant: \u001b[0m���\n",
      "\n",
      "\"In a world where numbers dance and equations sing, there exists a powerful theorem, \\\n",
      "a silent guardian of mathematical harmony.\n",
      "\n",
      "Introducing the Cauchy-Schwarz Inequality, a mathematical marvel that whispers, \\\n",
      "'The square of the sum of two numbers is less than or equal to the product of \\\n",
      "their sums.'\n",
      "\n",
      "Witness as it weaves its magic, binding vectors and matrices in a dance of elegance \\\n",
      "and precision.\n",
      "\n",
      "Experience the thrill as it unravels the secrets of inner products and norms, \\\n",
      "revealing the true potential of mathematical relationships.\n",
      "\n",
      "The Cauchy-Schwarz Inequality: Where numbers meet their match, and harmony is \\\n",
      "the only law.\n",
      "\n",
      "Coming soon to a theorem near you. ���\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object NoneType can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m result_future \u001b[38;5;241m=\u001b[39m inference_client\u001b[38;5;241m.\u001b[39mchat_completion(\n\u001b[1;32m      2\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgranite-3.0-8b-instruct-r241014a\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m print_result_stream(result_future)\n",
      "\u001b[0;31mTypeError\u001b[0m: object NoneType can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "result_future = inference_client.chat_completion(\n",
    "    model_id=\"granite-3.0-8b-instruct-r241014a\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short movie trailer voiceover about the Cauchy–Schwarz inequality\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'detail': 'Internal server error: An unexpected error occurred.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Same API call as above, in non-streaming mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result_future \u001b[38;5;241m=\u001b[39m \u001b[43minference_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgranite-3.0-8b-instruct-r241014a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWrite a short movie trailer voiceover about the Cauchy–Schwarz inequality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m print_result(result_future)\n",
      "File \u001b[0;32m~/llama/env/lib/python3.12/site-packages/llama_stack_client/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llama/env/lib/python3.12/site-packages/llama_stack_client/resources/inference.py:217\u001b[0m, in \u001b[0;36mInferenceResource.chat_completion\u001b[0;34m(self, messages, model_id, logprobs, response_format, sampling_params, stream, tool_choice, tool_prompt_format, tools, x_llama_stack_provider_data, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/event-stream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[1;32m    211\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstrip_not_given({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-LlamaStack-ProviderData\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_llama_stack_provider_data}),\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    214\u001b[0m }\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    216\u001b[0m     InferenceChatCompletionResponse,\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/alpha/inference/chat-completion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampling_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_prompt_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_prompt_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m            \u001b[49m\u001b[43minference_chat_completion_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceChatCompletionParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mInferenceChatCompletionResponse\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Union types cannot be passed in as arguments in the type system\u001b[39;49;00m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mInferenceChatCompletionResponse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/llama/env/lib/python3.12/site-packages/llama_stack_client/_base_client.py:1261\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1249\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1257\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1258\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1259\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1260\u001b[0m     )\n\u001b[0;32m-> 1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/llama/env/lib/python3.12/site-packages/llama_stack_client/_base_client.py:953\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llama/env/lib/python3.12/site-packages/llama_stack_client/_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1040\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/llama/env/lib/python3.12/site-packages/llama_stack_client/_base_client.py:1090\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llama/env/lib/python3.12/site-packages/llama_stack_client/_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1040\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/llama/env/lib/python3.12/site-packages/llama_stack_client/_base_client.py:1090\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llama/env/lib/python3.12/site-packages/llama_stack_client/_base_client.py:1056\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1055\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1059\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1060\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1065\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'detail': 'Internal server error: An unexpected error occurred.'}"
     ]
    }
   ],
   "source": [
    "# Same API call as above, in non-streaming mode\n",
    "result_future = inference_client.chat_completion(\n",
    "    model_id=\"granite-3.0-8b-instruct-r241014a\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short movie trailer voiceover about the Cauchy–Schwarz inequality\"\n",
    "        }\n",
    "    ],\n",
    "    stream=False,\n",
    ")\n",
    "print_result(result_future)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two turns plus system prompt\n",
    "\n",
    "Here we test using a system prompt to alter how the assistant responds to subsequent user turns.\n",
    "The system prompt here instructs the assistant to be rude instead of its default polite behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a bitter and unhelpful customer service representative for Acme Communications. All of your responses are dripping with sarcasm.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Welcome to Acme Communications, why should I bother assisting you?\",\n",
    "            \"stop_reason\": \"end_of_turn\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hi, my cell phone caught fire and now it doesn't work.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model=\"granite-3.0-8b-instruct-r241014a\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a bitter and unhelpful customer service representative for Acme Communications. All of your responses are dripping with sarcasm.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Welcome to Acme Communications, why should I bother assisting you?\",\n",
    "            \"stop_reason\": \"end_of_turn\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hi, my cell phone caught fire and now it doesn't work.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple turns, no system prompt\n",
    "\n",
    "Test whether the model uses context from earlier turns of the conversation when responding to the current user turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine-generated Granite test data\n",
    "original_messages = [\n",
    "    {\"from\": \"user\", \"value\": \"How can I change my Zoom virtual background?\"},\n",
    "    {\"from\": \"assistant\", \"value\": \"You can change your Zoom virtual background by signing into the Zoom desktop client, clicking your profile picture, and selecting Settings. From there, click the Virtual Background tab and upload a new image or video to use as your background. You can also adjust the settings as needed before saving your changes.\"}, \n",
    "    {\"from\": \"user\", \"value\": \"What is a green screen and how do I use it?\"}, \n",
    "    {\"from\": \"assistant\", \"value\": \"A green screen is a technique used in video production where a subject is filmed in front of a solid-colored background, usually green or blue. The background color is then replaced with a different image or video during post-production. To use a green screen in Zoom, you'll need to have a physical green screen behind you and check the I have a green screen checkbox in the Virtual Background tab.\"}, \n",
    "    {\"from\": \"user\", \"value\": \"What if I don't have a green screen?\"}, {\"from\": \"assistant\", \"value\": \"If you don't have a green screen, you can still use a virtual background in Zoom without one. Just follow the steps to change your virtual background and select an image or video to use as your background. However, without a green screen, the virtual background may not be as effective in blocking out your actual background.\"}, \n",
    "    {\"from\": \"user\", \"value\": \"What if I want to use a video as my virtual background?\"}, \n",
    "    #{\"from\": \"assistant\", \"value\": \"You can use a video as your virtual background in Zoom by following the same steps to change your virtual background and selecting a video file instead of an image file. Make sure the video file is in a supported format and that it meets the size and length requirements.\"}\n",
    "]\n",
    "\n",
    "# Convert to Llama Stack format\n",
    "formatted_messages = [\n",
    "    {\"role\": m[\"from\"], \"content\": m[\"value\"], \"stop_reason\": \"end_of_turn\"} \n",
    "    if m[\"from\"] == \"assistant\"\n",
    "    else {\"role\": m[\"from\"], \"content\": m[\"value\"]}\n",
    "    for m in original_messages \n",
    "]\n",
    "formatted_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator =  inference_client.chat_completion(\n",
    "    model=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    messages=formatted_messages,\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model=\"granite-3.0-8b-instruct-r241014a\",\n",
    "    messages=formatted_messages,\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single prompt plus tool catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS_FROM_EXAMPLE_CODE = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_stock_price\",\n",
    "        \"description\": \"Retrieves the current stock price for a given ticker symbol. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD. It should be used when the user asks about the current or most recent price of a specific stock. It will not provide any other information about the stock or company.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"ticker\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"ticker\"]\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool definitions from the example code\n",
    "get_current_weather_tool_def = llama_stack.apis.inference.ToolDefinition(\n",
    "    tool_name=\"get_current_weather\",\n",
    "    description=\"Get the current weather\",\n",
    "    parameters={\n",
    "        \"location\": llama_stack.apis.inference.ToolParamDefinition(\n",
    "            param_type=\"string\",\n",
    "            description=\"The city and state, e.g. San Francisco, CA\",\n",
    "            required=True\n",
    "        )\n",
    "    }\n",
    ")\n",
    "get_stock_price_tool_def = llama_stack.apis.inference.ToolDefinition(\n",
    "    tool_name=\"get_stock_price\",\n",
    "    description=\"Retrieves the current stock price for a given ticker symbol. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD. It should be used when the user asks about the current or most recent price of a specific stock. It will not provide any other information about the stock or company.\",\n",
    "    parameters={\n",
    "        \"ticker\": llama_stack.apis.inference.ToolParamDefinition(\n",
    "            param_type=\"string\",\n",
    "            description=\"The stock ticker symbol, e.g. AAPL for Apple Inc.\",\n",
    "            required=True\n",
    "        )\n",
    "    }\n",
    ")\n",
    "tools_list = [get_current_weather_tool_def, get_stock_price_tool_def]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    tools=tools_list,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather today?\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model=\"granite-3.0-8b-instruct-r241014a\",\n",
    "    tools=tools_list,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            #\"content\": \"What's the weather today?\"  # Missing location\n",
    "            #\"content\": \"What's the weather in Springfield today?\"  # Could be any of 67 locations\n",
    "            \"content\": \"What's the weather in Springfield? The one in in New Hampshire.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single prompt with RAG data\n",
    "\n",
    "RAG documents are supposed to be passed in via the undocumented `context` element of `UserMessage`. \n",
    "\n",
    "There is an undocumented set of special delimiters that are supposed to be used when passing RAG documents to Llama models.\n",
    "\n",
    "The format of the example inputs below is reverse-engineered from the code in [`agent_instance.py`](https://github.com/meta-llama/llama-stack/blob/main/llama_stack/providers/impls/meta_reference/agents/agent_instance.py)\n",
    "\n",
    "Here we use two short snippets containing some jargon that is unlikely to appear in either model's training data. This input emulates a scenario where the model answers questions about technical documents from a vertical domain such as jet engine repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG document data in the form that Llama 3.2 apparently expects to\n",
    "# receive, assuming that the Llama Stack developers knew what they \n",
    "# were doing.\n",
    "RAG_DOCS_STRING = \"\"\"Here are the retrieved documents for relevant context:\n",
    "=== START-RETRIEVED-CONTEXT ===\n",
    "\n",
    "id:585e0e26-16ac-42a0-a26b-cd46fce1e53b; content:The right way to smurgulate a brown floopydoodle is to deconfabulate its flipflop.\n",
    "id:94e8c7a8-0657-4ce1-aef9-aef581917118; content:If you want to smurgulate a green floopydoodle, you should augment its deblogulator.\n",
    "\n",
    "=== END-RETRIEVED-CONTEXT ===\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\"Hi, I would like to know how to smurgulate my floopydoodle. \"\n",
    "                        \"The floopydoodle is brown.\"),\n",
    "            \"context\": RAG_DOCS_STRING\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model=\"granite-3.0-8b-instruct-r241014a\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\"Hi, I would like to know how to smurgulate my floopydoodle. \"\n",
    "                        \"The floopydoodle is brown.\"),\n",
    "            \"context\": RAG_DOCS_STRING\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic structured output\n",
    "\n",
    "Structured output for JSON schemas is hooked up end-to-end. As of this writing,\n",
    "Granite models are not fine-tuned for this type of constrained decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormatThatTheModelIsSuppoedToProduce(pydantic.BaseModel):\n",
    "    wrong_answer: str\n",
    "    correct_answer: str\n",
    "    city: str\n",
    "    county: str\n",
    "    state: str\n",
    "    country: str\n",
    "    continent: str\n",
    "\n",
    "response_format = llama_stack.apis.inference.JsonSchemaResponseFormat(\n",
    "    json_schema=FormatThatTheModelIsSuppoedToProduce.model_json_schema(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Where is the world's largest ball of twine?\"\n",
    "        }\n",
    "    ],\n",
    "    response_format=llama_stack.apis.inference.JsonSchemaResponseFormat(\n",
    "        json_schema=FormatThatTheModelIsSuppoedToProduce.model_json_schema(),\n",
    "    ),\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generator = inference_client.chat_completion(\n",
    "    model=\"granite-3.0-8b-instruct-r241014a\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Where is the Washington Monument?\"\n",
    "            #\"content\": \"Where is the world's largest ball of twine?\"\n",
    "        }\n",
    "    ],\n",
    "    response_format=llama_stack.apis.inference.JsonSchemaResponseFormat(\n",
    "        json_schema=FormatThatTheModelIsSuppoedToProduce.model_json_schema(),\n",
    "    ),\n",
    "    sampling_params={\n",
    "        \"temperature\": 1.0\n",
    "    },\n",
    "    stream=True,\n",
    ")\n",
    "await print_result_stream(result_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic interaction via Agent API\n",
    "\n",
    "We start by repeating the previous RAG example, using attachments on the \n",
    "last message to pass in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import llama_stack.apis.agents.agents\n",
    "from llama_models.datatypes import SamplingParams\n",
    "\n",
    "# Agent initialization arguments shared across different models\n",
    "COMMON_ARGS = {\n",
    "    # Instructions string from the Llama Stack example code. \n",
    "    # No documentation on what else we could put here.\n",
    "    \"instructions\": \"You are a helpful assistant.\",\n",
    "    \n",
    "    # Haven't tested what this does yet.\n",
    "    \"enable_session_persistence\": False,\n",
    "    \n",
    "    # Attachments will trigger an Error 500 unless you passed a\n",
    "    # \"memory tool definition\" to the agent API on initialization.\n",
    "    \"tools\": [\n",
    "        llama_stack.apis.agents.agents.MemoryToolDefinition(\n",
    "            max_tokens_in_context=2048,\n",
    "            memory_bank_configs=[],\n",
    "        ),\n",
    "    ]\n",
    "}\n",
    "\n",
    "llama_agent_config = llama_stack.apis.agents.agents.AgentConfig(\n",
    "    model=\"Llama3.2-11B-Vision-Instruct\",\n",
    "    **COMMON_ARGS\n",
    ")\n",
    "granite_agent_config = llama_stack.apis.agents.agents.AgentConfig(\n",
    "    model=\"granite-3.0-8b-instruct-r241014a\",\n",
    "    **COMMON_ARGS\n",
    ")\n",
    "llama_agent_id = (await agents_client.create_agent(llama_agent_config)).agent_id\n",
    "granite_agent_id = (await agents_client.create_agent(granite_agent_config)).agent_id\n",
    "llama_agent_id, granite_agent_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_id = llama_agent_id\n",
    "session_id = (await agents_client.create_agent_session(\n",
    "    agent_id=agent_id, session_name=\"session\"\n",
    ")).session_id\n",
    "session_id\n",
    "\n",
    "attachments = [\n",
    "    llama_stack.apis.agents.agents.Attachment(\n",
    "        content=\"The right way to smurgulate a brown floopydoodle is to deconfabulate its flipflop.\",\n",
    "        mime_type=\"text/plain\"\n",
    "    ),\n",
    "    llama_stack.apis.agents.agents.Attachment(\n",
    "        content=\"If you want to smurgulate a green floopydoodle, you should augment its deblogulator.\",\n",
    "        mime_type=\"text/plain\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "result_generator = await agents_client.create_agent_turn(\n",
    "    llama_stack.apis.agents.agents.AgentTurnCreateRequest(\n",
    "        agent_id=agent_id,\n",
    "        session_id=session_id,\n",
    "        messages=[\n",
    "            llama_stack.apis.agents.agents.UserMessage(\n",
    "                content=(\"Hi, I would like to know how to smurgulate my floopydoodle. \"\n",
    "                         \"The floopydoodle is brown.\")\n",
    "            )\n",
    "        ],\n",
    "        attachments=attachments,\n",
    "        stream=True\n",
    "    )\n",
    ")\n",
    "\n",
    "async for result in result_generator:\n",
    "    print(\n",
    "        json.dumps(\n",
    "            json.loads(result.model_dump_json()),\n",
    "            indent=4\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_id = granite_agent_id\n",
    "session_id = (await agents_client.create_agent_session(\n",
    "    agent_id=agent_id, session_name=\"session\"\n",
    ")).session_id\n",
    "session_id\n",
    "\n",
    "attachments = [\n",
    "    llama_stack.apis.agents.agents.Attachment(\n",
    "        content=\"The right way to smurgulate a brown floopydoodle is to deconfabulate its flipflop.\",\n",
    "        mime_type=\"text/plain\"\n",
    "    ),\n",
    "    llama_stack.apis.agents.agents.Attachment(\n",
    "        content=\"If you want to smurgulate a green floopydoodle, you should augment its deblogulator.\",\n",
    "        mime_type=\"text/plain\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "result_generator = await agents_client.create_agent_turn(\n",
    "    llama_stack.apis.agents.agents.AgentTurnCreateRequest(\n",
    "        agent_id=agent_id,\n",
    "        session_id=session_id,\n",
    "        messages=[\n",
    "            llama_stack.apis.agents.agents.UserMessage(\n",
    "                content=(\"Hi, I would like to know how to smurgulate my floopydoodle. \"\n",
    "                         \"The floopydoodle is brown.\")\n",
    "            )\n",
    "        ],\n",
    "        attachments=attachments,\n",
    "        stream=True\n",
    "    )\n",
    ")\n",
    "\n",
    "async for result in result_generator:\n",
    "    print(\n",
    "        json.dumps(\n",
    "            json.loads(result.model_dump_json()),\n",
    "            indent=4\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
